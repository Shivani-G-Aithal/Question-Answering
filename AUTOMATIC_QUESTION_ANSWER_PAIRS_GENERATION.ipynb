{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AUTOMATIC QUESTION-ANSWER PAIRS GENERATION.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1_zW5mIy7p98peQcWgSEWXC1DuvDPdxIm",
      "authorship_tag": "ABX9TyOhh8rQ+8dt2vU9ghxhAngP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shivani-G-Aithal/Question-Answering/blob/master/AUTOMATIC_QUESTION_ANSWER_PAIRS_GENERATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiBQi0pNwdOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "24dd7269-c851-40b2-8e95-2c7bfa430a30"
      },
      "source": [
        "#installing dependencies required for prophetnet and Bert\n",
        "!pip install fairseq==v0.9.0D \n",
        "!pip install transformers\n",
        "!pip install pytorch_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement fairseq==v0.9.0D (from versions: 0.6.1, 0.6.2, 0.7.1, 0.7.2, 0.8.0, 0.9.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for fairseq==v0.9.0D\u001b[0m\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.5.1+cu101)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.14.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch_transformers) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.20 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.17.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->pytorch_transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->pytorch_transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT2gM8e8ze2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertForQuestionAnswering\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5_OL-fyig3W",
        "colab_type": "text"
      },
      "source": [
        "**PROPHETNET PREPROCESSING STEPS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Hc87yBg6N4",
        "colab_type": "text"
      },
      "source": [
        "1.CONVERT INPUT PASSAGE INTO TOKENIZED BERT-CASED FORMAT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.CONVERT TOKENIZED BERT-CASED INPUT PASSAGE INTO TOKENIZED BERT-UNCASED FORMAT\n",
        "\n",
        "3.CONVERT TOKENIZED BERT-UNCASED PASSAGE INTO BINARIES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YnvW58AWfcZg"
      },
      "source": [
        "*1.PROGRAM TO TOKENIZE INPUT PASSAGE INTO BERT-CASED FORMAT.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obYlcxhpfcZo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cc42d4a-a391-45d8-97f2-12d15f544331"
      },
      "source": [
        "cd /content/drive/My Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d1PFblE4fcZ_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ca39d3a8-3197-4780-97eb-2b9d30304978"
      },
      "source": [
        "#using UNILMTOKENIZER BERT-CASED (bert can also be used here since unilmtokenizer inherits bert tokenizer and also in prophetnet page they have specified so I have used it.)\n",
        "from transformers.tokenization_bert import BertTokenizer, whitespace_tokenize\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from io import open\n",
        "logger = logging.getLogger(__name__)\n",
        "VOCAB_FILES_NAMES = {'vocab_file': 'vocab.txt'}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    'vocab_file':\n",
        "    {\n",
        "        'unilm-large-cased': \"https://unilm.blob.core.windows.net/ckpt/unilm-large-cased-vocab.txt\",\n",
        "        'unilm-base-cased': \"https://unilm.blob.core.windows.net/ckpt/unilm-base-cased-vocab.txt\",\n",
        "        'unilm1-large-cased': \"https://unilm.blob.core.windows.net/ckpt/unilm1-large-cased-vocab.txt\",\n",
        "        'unilm1-base-cased': \"https://unilm.blob.core.windows.net/ckpt/unilm1-base-cased-vocab.txt\",\n",
        "        'unilm1.2-base-uncased': \"https://unilm.blob.core.windows.net/ckpt/unilm1.2-base-uncased-vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    'unilm-large-cased': 512,\n",
        "    'unilm-base-cased': 512,\n",
        "    'unilm1-large-cased': 512,\n",
        "    'unilm1-base-cased': 512,\n",
        "    'unilm1.2-base-uncased': 512,\n",
        "}\n",
        "\n",
        "\n",
        "class UnilmTokenizer(BertTokenizer):\n",
        "    r\"\"\"\n",
        "    Constructs a UnilmTokenizer.\n",
        "    :class:`~transformers.UnilmTokenizer` is identical to BertTokenizer and runs end-to-end tokenization: punctuation splitting + wordpiece\n",
        "    Args:\n",
        "        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
        "        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n",
        "        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
        "        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
        "            minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
        "        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
        "            do_wordpiece_only=False\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = 'unilm-base-cased-vocab.txt'\n",
        "    pretrained_vocab_files_map = 'unilm-base-cased'\n",
        "    max_model_input_sizes = 512\n",
        "\n",
        "\n",
        "class WhitespaceTokenizer(object):\n",
        "    def tokenize(self, text):\n",
        "        return whitespace_tokenize(text)\n",
        "#loading unilm vocab files\n",
        "!wget https://unilm.blob.core.windows.net/ckpt/unilm-base-cased-vocab.txt\n",
        "tokenizer = UnilmTokenizer('unilm-base-cased-vocab.txt.2',word_piece=False , do_basic_tokenize=True, never_split=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-17 04:54:08--  https://unilm.blob.core.windows.net/ckpt/unilm-base-cased-vocab.txt\n",
            "Resolving unilm.blob.core.windows.net (unilm.blob.core.windows.net)... 52.239.193.100\n",
            "Connecting to unilm.blob.core.windows.net (unilm.blob.core.windows.net)|52.239.193.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 213419 (208K) [text/plain]\n",
            "Saving to: ‘unilm-base-cased-vocab.txt.12’\n",
            "\n",
            "unilm-base-cased-vo 100%[===================>] 208.42K   552KB/s    in 0.4s    \n",
            "\n",
            "2020-07-17 04:54:09 (552 KB/s) - ‘unilm-base-cased-vocab.txt.12’ saved [213419/213419]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_gPNDuOofcaS",
        "colab": {}
      },
      "source": [
        "#Enter the input paragraph\n",
        "text=\"\"\"\n",
        "On 18 November 2015, Sky announced Sky Q, a range of products and services to be available in 2016. The Sky Q range consists of three set top boxes (Sky Q, Sky Q Silver and Sky Q Mini), a broadband router (Sky Q Hub) and mobile applications. The Sky Q set top boxes introduce a new user interface, Wi-Fi hotspot functionality, Power-line and Bluetooth connectivity and a new touch-sensitive remote control. The Sky Q Mini set top boxes connect to the Sky Q Silver set top boxes with a Wi-Fi or Power-line connection rather than receive their own satellite feeds. This allows all set top boxes in a household to share recordings and other media. The Sky Q Silver set top box is capable of receiving and displaying UHD broadcasts, which Sky will introduce later in 2016.\n",
        "\"\"\"\n",
        "#code to split the sentences and add [SEP] token at the end.\n",
        "x=tokenizer.tokenize(text) \n",
        "y=str(' '.join(x))\n",
        "z=y+'[SEP]'\n",
        "\n",
        "#Named Entities\n",
        "doc = nlp(text)\n",
        "k=[tokenizer.tokenize(X.text) for X in doc.ents]\n",
        "\n",
        "\n",
        "f = open('/content/drive/My Drive/ques/unilmtokenized/passage.txt', 'w')\n",
        "for i in range(len(k)):\n",
        "  out=z+str(k[i])+\"\\n\"\n",
        "  f.write(str(out))\n",
        "f.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJB0G55gfx_e",
        "colab_type": "text"
      },
      "source": [
        "**BERTCASED INPUT FORMATTING IS DONE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mtnJFV4gFqX",
        "colab_type": "text"
      },
      "source": [
        "*2.NOW CONVERTING BERT-CASED TO BERT-UNCASED*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK2SeJi06GgN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab6d9c7a-0168-47cc-ff0a-a26c47bcbf1c"
      },
      "source": [
        "cd /content/drive/My Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5a7R8P8PD8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b7cea7f4-a641-45f4-9f27-f927441212de"
      },
      "source": [
        "#script which converts BERT-cased PASSAGE to  BERT-uncased PASSAGE\n",
        "from pytorch_transformers import BertTokenizer\n",
        "import tqdm\n",
        "def convert_cased2uncased(fin, fout):\n",
        "    fin = open(fin, 'r', encoding='utf-8')\n",
        "    fout = open(fout, 'w', encoding='utf-8')\n",
        "    tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    for line in tqdm.tqdm(fin.readlines()):\n",
        "        org = line.strip().replace(\" ##\", \"\")\n",
        "        new = tok.tokenize(org)\n",
        "        new_line = \" \".join(new)\n",
        "        fout.write('{}\\n'.format(new_line))\n",
        "convert_cased2uncased('ques/unilmtokenized/test.q.tok.txt', 'ques/prophetnet_tokenized/passage.tgt')\n",
        "\n",
        "def convert_cased2uncased_reverse(fin, fout):\n",
        "    fin = open(fin, 'r', encoding='utf-8')\n",
        "    fout = open(fout, 'w', encoding='utf-8')\n",
        "    tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    for line in tqdm.tqdm(fin.readlines()):\n",
        "        org = line.strip().replace(\" ##\", \"\").split(\"[SEP]\")\n",
        "        ans = tok.tokenize(org[1].strip())\n",
        "        par = tok.tokenize(org[0].strip())[:510 - len(ans)] # at most 512 tokens can be fed to our model\n",
        "        par = \" \".join(par)\n",
        "        ans = \" \".join(ans)\n",
        "        fout.write('{} [SEP] {}\\n'.format(ans, par))\n",
        "convert_cased2uncased_reverse('ques/unilmtokenized/passage.txt', 'ques/prophetnet_tokenized/passage.src')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 4646760.38B/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 2324.19it/s]\n",
            "100%|██████████| 15/15 [00:00<00:00, 380.74it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07QRo8q2WdAQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aefd94fb-5708-4fa2-b470-5cc73cd33fde"
      },
      "source": [
        "cd /content/drive/My Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPAZ3oSciASo",
        "colab_type": "text"
      },
      "source": [
        "*3. FINALLY CONVERTING BERT-UNCASED INTO BINARIES*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UrDn4uVpS4DM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "3ab5ad04-dd6e-4102-82d6-248405c1fd7d"
      },
      "source": [
        "#converting BERT-uncased PASSAGE into Binaries to feed into the model\n",
        "!fairseq-preprocess \\\n",
        "--user-dir src/prophetnet \\\n",
        "--task translation_prophetnet \\\n",
        "--source-lang src --target-lang tgt \\\n",
        "--testpref ques/prophetnet_tokenized/passage \\\n",
        "--destdir questionsin/processed --srcdict vocab.txt --tgtdict vocab.txt \\\n",
        "--workers 20"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(align_suffix=None, alignfile=None, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='questionsin/processed', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='src', srcdict='vocab.txt', target_lang='tgt', task='translation_prophetnet', tensorboard_logdir='', testpref='ques/prophetnet_tokenized/passage', tgtdict='vocab.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, trainpref=None, user_dir='src/prophetnet', validpref=None, workers=20)\n",
            "| [src] Dictionary: 30521 types\n",
            "| [src] ques/prophetnet_tokenized/passage.src: 15 sents, 2755 tokens, 0.0% replaced by [UNK]\n",
            "| [tgt] Dictionary: 30521 types\n",
            "| [tgt] ques/prophetnet_tokenized/passage.tgt: 30 sents, 426 tokens, 0.0% replaced by [UNK]\n",
            "| Wrote preprocessed data to questionsin/processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCSH6cwViP7O",
        "colab_type": "text"
      },
      "source": [
        "**GENERATE QUESTIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqYS9VdWvxDI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3640fb8a-f2ca-4ea5-85a1-34472173c5b3"
      },
      "source": [
        "#generate questions for the input passage\n",
        "BEAM=5\n",
        "LENPEN=1.5\n",
        "CHECK_POINT='ques/prophetnet_large_16G_qg_model.pt'\n",
        "OUTPUT_FILE='ques/output_ck5_pelt1.5_test_beam5.txt'\n",
        "\n",
        "!fairseq-generate questionsin/processed --path $CHECK_POINT --user-dir src/prophetnet --task translation_prophetnet --batch-size 80 --gen-subset test --beam $BEAM --num-workers 4 --no-repeat-ngram-size 3 --lenpen $LENPEN 2>&1 > $OUTPUT_FILE\n",
        "#format the output\n",
        "!grep ^H ques/output_ck5_pelt1.5_test_beam5.txt | cut -c 3- | sort -n | cut -f3- | sed \"s/ ##//g\" > ques/sort_hypo_ck5_pelt1.5_test_beam5.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0% 0/1 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/BinaryOps.cpp:66: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLuDLwttHA6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def answer_question(question, answer_text):\n",
        "    '''\n",
        "    Takes a `question` string and an `answer_text` string (which contains the\n",
        "    answer), and identifies the words within the `answer_text` that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    # ======== Tokenize ========\n",
        "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        "\n",
        "    # Report how long the input sequence is.\n",
        "    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
        "\n",
        "    # ======== Set Segment IDs ========\n",
        "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "    # The number of segment A tokens includes the [SEP] token istelf.\n",
        "    num_seg_a = sep_index + 1\n",
        "\n",
        "    # The remainder are segment B.\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "    # Construct the list of 0s and 1s.\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "    # There should be a segment_id for every input token.\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "\n",
        "    # ======== Evaluate ========\n",
        "    # Run our example question through the model.\n",
        "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
        "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
        "\n",
        "    # ======== Reconstruct Answer ========\n",
        "    # Find the tokens with the highest `start` and `end` scores.\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "\n",
        "    # Get the string versions of the input tokens.\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    # Start with the first token.\n",
        "    answer = tokens[answer_start]\n",
        "\n",
        "    # Select the remaining answer tokens and join them with whitespace.\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        \n",
        "        # If it's a subword token, then recombine it with the previous token.\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        \n",
        "        # Otherwise, add a space then the token.\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErzQUkUVNefV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeC0OJ3TOLez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6619625c-4606-4c3a-ac16-d13271f2cdeb"
      },
      "source": [
        "import textwrap\n",
        "# Wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "print('PASSAGE:')\n",
        "print(wrapper.fill(text))\n",
        "print(\"\\n\")\n",
        "print('AUTOMATIC QUESTION-ANSWER PAIRS GENERATION')\n",
        "print(\"\\n\")\n",
        "f = open('/content/drive/My Drive/ques/sort_hypo_ck5_pelt1.5_test_beam5.txt', 'r')\n",
        "for i in f:\n",
        "  print(i)\n",
        "  answer_question(i, text)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PASSAGE:\n",
            " On 18 November 2015, Sky announced Sky Q, a range of products and services to\n",
            "be available in 2016. The Sky Q range consists of three set top boxes (Sky Q,\n",
            "Sky Q Silver and Sky Q Mini), a broadband router (Sky Q Hub) and mobile\n",
            "applications. The Sky Q set top boxes introduce a new user interface, Wi-Fi\n",
            "hotspot functionality, Power-line and Bluetooth connectivity and a new touch-\n",
            "sensitive remote control. The Sky Q Mini set top boxes connect to the Sky Q\n",
            "Silver set top boxes with a Wi-Fi or Power-line connection rather than receive\n",
            "their own satellite feeds. This allows all set top boxes in a household to share\n",
            "recordings and other media. The Sky Q Silver set top box is capable of receiving\n",
            "and displaying UHD broadcasts, which Sky will introduce later in 2016.\n",
            "\n",
            "\n",
            "AUTOMATIC QUESTION-ANSWER PAIRS GENERATION\n",
            "\n",
            "\n",
            "what are some of the hashtags for sky q ?\n",
            "\n",
            "Answer: \"sky q , sky q silver and sky q mini\"\n",
            "\n",
            "\n",
            "who announced the sky q range of products and services ?\n",
            "\n",
            "Answer: \"sky\"\n",
            "\n",
            "\n",
            "what are the names of sky ' s products and services ?\n",
            "\n",
            "Answer: \"sky q\"\n",
            "\n",
            "\n",
            "what year did sky announce the sky q range of products ?\n",
            "\n",
            "Answer: \"2015\"\n",
            "\n",
            "\n",
            "how many set top boxes are in the sky q range ?\n",
            "\n",
            "Answer: \"three\"\n",
            "\n",
            "\n",
            "what are the names of sky ' s set top boxes ?\n",
            "\n",
            "Answer: \"sky q , sky q silver and sky q mini\"\n",
            "\n",
            "\n",
            "what are the names of sky ' s products ?\n",
            "\n",
            "Answer: \"sky q , sky q silver and sky q mini\"\n",
            "\n",
            "\n",
            "what does the acronym sky q stand for ?\n",
            "\n",
            "Answer: \"sky q , a range of products and services to be available in 2016\"\n",
            "\n",
            "\n",
            "what are the names of sky ' s set top boxes ?\n",
            "\n",
            "Answer: \"sky q , sky q silver and sky q mini\"\n",
            "\n",
            "\n",
            "what are the names of sky ' s set top boxes ?\n",
            "\n",
            "Answer: \"sky q , sky q silver and sky q mini\"\n",
            "\n",
            "\n",
            "what does the sky q name stand for ?\n",
            "\n",
            "Answer: \"a range of products and services to be available in 2016\"\n",
            "\n",
            "\n",
            "what are the names of sky ' s set top boxes ?\n",
            "\n",
            "Answer: \"sky q , sky q silver and sky q mini\"\n",
            "\n",
            "\n",
            "what does sky call uhd broadcasts ?\n",
            "\n",
            "Answer: \"sky q silver set top box is capable of receiving and displaying uhd broadcasts , which sky will introduce later in 2016\"\n",
            "\n",
            "\n",
            "who announced the sky q range of products and services ?\n",
            "\n",
            "Answer: \"sky\"\n",
            "\n",
            "\n",
            "what year did sky announce the sky q range of products ?\n",
            "\n",
            "Answer: \"2015\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}